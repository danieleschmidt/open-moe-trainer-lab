"""
Self-Improving MoE Framework - Autonomous Enhancement Module
Implements adaptive learning patterns that evolve based on usage data.
"""

import json
import time
import math
import threading
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import statistics

import torch
import torch.nn as nn


@dataclass
class AdaptationMetrics:
    """Metrics for tracking adaptation effectiveness."""
    accuracy_improvement: float = 0.0
    efficiency_gain: float = 0.0
    load_balance_improvement: float = 0.0
    routing_confidence_delta: float = 0.0
    timestamp: float = 0.0
    

@dataclass
class PerformancePattern:
    """Pattern recognition for performance optimization."""
    pattern_type: str
    trigger_conditions: Dict[str, Any]
    adaptation_strategy: str
    success_rate: float
    usage_count: int


class AdaptiveRouter(nn.Module):
    """Self-improving router that adapts based on performance feedback."""
    
    def __init__(
        self,
        hidden_size: int,
        num_experts: int,
        adaptation_rate: float = 0.001,
        memory_window: int = 1000,
        confidence_threshold: float = 0.8
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.adaptation_rate = adaptation_rate
        self.memory_window = memory_window
        self.confidence_threshold = confidence_threshold
        
        # Core router parameters
        self.router_weights = nn.Parameter(torch.randn(hidden_size, num_experts) * 0.02)
        
        # Adaptive components
        self.performance_history = deque(maxlen=memory_window)
        self.adaptation_patterns = {}
        self.routing_success_rates = torch.zeros(num_experts)
        self.expert_specialization_scores = torch.zeros(num_experts)
        
        # Self-improvement state
        self.adaptation_count = 0
        self.last_adaptation_time = time.time()
        self.improvement_trajectory = []
        
        # Pattern recognition
        self.load_pattern_detector = LoadPatternDetector()
        self.efficiency_optimizer = EfficiencyOptimizer()
        
    def forward(self, hidden_states: torch.Tensor, target_task: Optional[str] = None) -> tuple:
        """Adaptive forward pass with self-improvement."""
        batch_size, seq_len, hidden_size = hidden_states.shape
        
        # Compute base router logits
        router_logits = torch.matmul(hidden_states, self.router_weights)
        
        # Apply dynamic adaptations
        if self.should_adapt():
            router_logits = self.apply_adaptive_adjustments(router_logits, hidden_states)
        
        # Track performance for learning
        performance_metrics = self.track_routing_performance(router_logits, hidden_states)
        self.performance_history.append(performance_metrics)
        
        # Trigger adaptation if needed
        if self.should_trigger_adaptation():
            self.perform_adaptation()
        
        return router_logits, performance_metrics
    
    def should_adapt(self) -> bool:
        """Determine if adaptation should be applied based on learned patterns."""
        if len(self.performance_history) < 10:
            return False
            
        recent_performance = list(self.performance_history)[-10:]
        avg_confidence = statistics.mean(p['routing_confidence'] for p in recent_performance)
        
        return avg_confidence < self.confidence_threshold
    
    def apply_adaptive_adjustments(self, router_logits: torch.Tensor, hidden_states: torch.Tensor) -> torch.Tensor:
        """Apply learned adaptations to improve routing decisions."""
        # Detect current load pattern
        current_pattern = self.load_pattern_detector.detect_pattern(router_logits)
        
        # Apply pattern-specific optimizations
        if current_pattern in self.adaptation_patterns:
            pattern_config = self.adaptation_patterns[current_pattern]
            adjustment = self.compute_pattern_adjustment(pattern_config, router_logits)
            router_logits = router_logits + adjustment
        
        # Apply expert specialization bias
        specialization_bias = self.expert_specialization_scores.unsqueeze(0).unsqueeze(0)
        router_logits = router_logits + 0.1 * specialization_bias
        
        return router_logits
    
    def track_routing_performance(self, router_logits: torch.Tensor, hidden_states: torch.Tensor) -> Dict[str, float]:
        """Track performance metrics for self-improvement."""
        with torch.no_grad():
            # Compute routing confidence
            probs = torch.softmax(router_logits, dim=-1)
            max_probs = torch.max(probs, dim=-1)[0]
            routing_confidence = torch.mean(max_probs).item()
            
            # Compute load balance
            expert_loads = torch.mean(probs, dim=[0, 1])
            target_load = 1.0 / self.num_experts
            load_variance = torch.var(expert_loads).item()
            
            # Compute routing entropy
            entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1)
            avg_entropy = torch.mean(entropy).item()
            
            return {
                'routing_confidence': routing_confidence,
                'load_variance': load_variance,
                'entropy': avg_entropy,
                'timestamp': time.time()
            }
    
    def should_trigger_adaptation(self) -> bool:
        """Determine if it's time to perform adaptation."""
        # Minimum time between adaptations
        if time.time() - self.last_adaptation_time < 10.0:
            return False
            
        # Sufficient data collected
        if len(self.performance_history) < 50:
            return False
            
        # Performance degradation detected
        if len(self.performance_history) >= 100:
            recent = list(self.performance_history)[-20:]
            older = list(self.performance_history)[-100:-80:]
            
            recent_confidence = statistics.mean(p['routing_confidence'] for p in recent)
            older_confidence = statistics.mean(p['routing_confidence'] for p in older)
            
            return recent_confidence < older_confidence - 0.05
        
        return False
    
    def perform_adaptation(self):
        """Execute self-improvement adaptation."""
        print(f"üîÑ Performing adaptive improvement (adaptation #{self.adaptation_count + 1})")
        
        # Analyze performance patterns
        patterns = self.analyze_performance_patterns()
        
        # Update expert specialization
        self.update_expert_specialization()
        
        # Learn new routing patterns
        self.learn_routing_patterns(patterns)
        
        # Apply efficiency optimizations
        optimizations = self.efficiency_optimizer.optimize(self.performance_history)
        self.apply_optimizations(optimizations)
        
        # Update adaptation state
        self.adaptation_count += 1
        self.last_adaptation_time = time.time()
        
        # Record improvement metrics
        improvement = self.measure_improvement()
        self.improvement_trajectory.append(improvement)
        
        print(f"‚úÖ Adaptation complete: {improvement.accuracy_improvement:.1%} accuracy improvement")
    
    def analyze_performance_patterns(self) -> List[PerformancePattern]:
        """Analyze recent performance to identify improvement patterns."""
        patterns = []
        
        if len(self.performance_history) < 20:
            return patterns
        
        # Group performance by similar conditions
        performance_groups = defaultdict(list)
        
        for perf in self.performance_history:
            # Categorize by load variance level
            if perf['load_variance'] < 0.01:
                category = 'balanced_load'
            elif perf['load_variance'] < 0.05:
                category = 'moderate_load' 
            else:
                category = 'imbalanced_load'
                
            performance_groups[category].append(perf)
        
        # Identify successful patterns
        for category, performances in performance_groups.items():
            if len(performances) >= 5:
                avg_confidence = statistics.mean(p['routing_confidence'] for p in performances)
                
                if avg_confidence > 0.8:  # Successful pattern
                    pattern = PerformancePattern(
                        pattern_type=category,
                        trigger_conditions={'load_variance_range': (0, 0.01) if category == 'balanced_load' else (0.01, 0.05)},
                        adaptation_strategy='maintain_balance' if category == 'balanced_load' else 'improve_balance',
                        success_rate=avg_confidence,
                        usage_count=len(performances)
                    )
                    patterns.append(pattern)
        
        return patterns
    
    def update_expert_specialization(self):
        """Update expert specialization scores based on performance."""
        if len(self.performance_history) < 10:
            return
            
        # Compute specialization based on routing success
        recent_performances = list(self.performance_history)[-20:]
        confidence_scores = [p['routing_confidence'] for p in recent_performances]
        
        # Higher confidence indicates better specialization
        avg_confidence = statistics.mean(confidence_scores)
        confidence_trend = confidence_scores[-1] - confidence_scores[0] if len(confidence_scores) > 1 else 0
        
        # Update specialization scores (simplified for demo)
        specialization_update = torch.tensor([
            0.1 * avg_confidence + 0.05 * confidence_trend for _ in range(self.num_experts)
        ])
        
        self.expert_specialization_scores += 0.1 * specialization_update
        self.expert_specialization_scores = torch.clamp(self.expert_specialization_scores, -1.0, 1.0)
    
    def learn_routing_patterns(self, patterns: List[PerformancePattern]):
        """Learn and store successful routing patterns."""
        for pattern in patterns:
            if pattern.success_rate > 0.75:  # Only learn from successful patterns
                self.adaptation_patterns[pattern.pattern_type] = {
                    'trigger_conditions': pattern.trigger_conditions,
                    'adaptation_strategy': pattern.adaptation_strategy,
                    'success_rate': pattern.success_rate,
                    'learned_at': time.time()
                }
    
    def compute_pattern_adjustment(self, pattern_config: Dict, router_logits: torch.Tensor) -> torch.Tensor:
        """Compute routing adjustments based on learned patterns."""
        strategy = pattern_config['adaptation_strategy']
        
        if strategy == 'maintain_balance':
            # Slight adjustment to maintain current good balance
            return torch.zeros_like(router_logits) * 0.01
        elif strategy == 'improve_balance':
            # Stronger adjustment to improve load balance
            current_probs = torch.softmax(router_logits, dim=-1)
            target_uniform = torch.ones_like(current_probs) / self.num_experts
            balance_adjustment = 0.1 * (target_uniform - current_probs)
            return balance_adjustment
        else:
            return torch.zeros_like(router_logits)
    
    def apply_optimizations(self, optimizations: Dict[str, Any]):
        """Apply efficiency optimizations learned from usage patterns."""
        if 'router_weight_adjustment' in optimizations:
            adjustment = optimizations['router_weight_adjustment']
            with torch.no_grad():
                self.router_weights += self.adaptation_rate * adjustment
    
    def measure_improvement(self) -> AdaptationMetrics:
        """Measure improvement from recent adaptation."""
        if len(self.performance_history) < 50:
            return AdaptationMetrics(timestamp=time.time())
        
        # Compare before and after adaptation
        pre_adaptation = list(self.performance_history)[-50:-25]
        post_adaptation = list(self.performance_history)[-25:]
        
        if not pre_adaptation or not post_adaptation:
            return AdaptationMetrics(timestamp=time.time())
        
        pre_confidence = statistics.mean(p['routing_confidence'] for p in pre_adaptation)
        post_confidence = statistics.mean(p['routing_confidence'] for p in post_adaptation)
        
        pre_load_var = statistics.mean(p['load_variance'] for p in pre_adaptation)
        post_load_var = statistics.mean(p['load_variance'] for p in post_adaptation)
        
        return AdaptationMetrics(
            accuracy_improvement=(post_confidence - pre_confidence) / max(pre_confidence, 0.01),
            load_balance_improvement=(pre_load_var - post_load_var) / max(pre_load_var, 0.001),
            routing_confidence_delta=post_confidence - pre_confidence,
            timestamp=time.time()
        )


class LoadPatternDetector:
    """Detects load distribution patterns for adaptive optimization."""
    
    def detect_pattern(self, router_logits: torch.Tensor) -> str:
        """Detect current load distribution pattern."""
        probs = torch.softmax(router_logits, dim=-1)
        expert_loads = torch.mean(probs, dim=[0, 1])
        
        variance = torch.var(expert_loads).item()
        max_load = torch.max(expert_loads).item()
        min_load = torch.min(expert_loads).item()
        
        if variance < 0.01:
            return 'balanced'
        elif max_load > 0.6:
            return 'concentrated'
        elif min_load < 0.05:
            return 'sparse'
        else:
            return 'moderate'


class EfficiencyOptimizer:
    """Optimizes routing efficiency based on performance patterns."""
    
    def optimize(self, performance_history: deque) -> Dict[str, Any]:
        """Generate efficiency optimizations based on performance data."""
        if len(performance_history) < 20:
            return {}
        
        recent_performances = list(performance_history)[-20:]
        
        # Analyze routing confidence trends
        confidence_scores = [p['routing_confidence'] for p in recent_performances]
        confidence_trend = statistics.linear_regression([i for i in range(len(confidence_scores))], confidence_scores).slope
        
        optimizations = {}
        
        # If confidence is declining, suggest router weight adjustment
        if confidence_trend < -0.01:
            # Generate adaptive adjustment (simplified for demo)
            adjustment_magnitude = abs(confidence_trend) * 10
            optimizations['router_weight_adjustment'] = torch.randn(32, 8) * adjustment_magnitude * 0.01
        
        return optimizations


class SelfImprovingMoEModel(nn.Module):
    """MoE Model with autonomous self-improvement capabilities."""
    
    def __init__(
        self,
        hidden_size: int = 768,
        num_experts: int = 8,
        num_layers: int = 6,
        adaptation_enabled: bool = True,
        improvement_logging: bool = True
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.num_layers = num_layers
        self.adaptation_enabled = adaptation_enabled
        self.improvement_logging = improvement_logging
        
        # Self-improving components
        self.adaptive_routers = nn.ModuleList([
            AdaptiveRouter(hidden_size, num_experts) for _ in range(num_layers)
        ])
        
        # Performance monitoring
        self.global_improvement_history = []
        self.adaptation_effectiveness = {}
        self.autonomous_learning_stats = {
            'total_adaptations': 0,
            'successful_adaptations': 0,
            'performance_improvements': [],
            'learning_velocity': 0.0
        }
        
        # Background learning thread
        self.learning_thread = None
        self.stop_learning = threading.Event()
        
        if adaptation_enabled:
            self.start_autonomous_learning()
    
    def forward(self, hidden_states: torch.Tensor, return_adaptation_info: bool = False):
        """Forward pass with autonomous adaptation tracking."""
        adaptation_info = {}
        layer_improvements = []
        
        current_states = hidden_states
        
        for i, adaptive_router in enumerate(self.adaptive_routers):
            # Apply adaptive routing
            router_logits, performance_metrics = adaptive_router(current_states)
            
            # Simulate expert processing (simplified for demo)
            expert_probs = torch.softmax(router_logits, dim=-1)
            # In real implementation, this would route to actual expert networks
            expert_output = torch.matmul(expert_probs.unsqueeze(-1), current_states.unsqueeze(-2)).squeeze(-2)
            current_states = expert_output
            
            if return_adaptation_info:
                layer_improvements.append({
                    'layer': i,
                    'adaptation_count': adaptive_router.adaptation_count,
                    'routing_confidence': performance_metrics['routing_confidence'],
                    'load_variance': performance_metrics['load_variance']
                })
        
        if return_adaptation_info:
            adaptation_info = {
                'layer_improvements': layer_improvements,
                'total_adaptations': sum(r.adaptation_count for r in self.adaptive_routers),
                'global_learning_stats': self.autonomous_learning_stats
            }
        
        return current_states, adaptation_info
    
    def start_autonomous_learning(self):
        """Start background autonomous learning process."""
        def learning_loop():
            while not self.stop_learning.wait(30):  # Check every 30 seconds
                self.perform_global_optimization()
        
        self.learning_thread = threading.Thread(target=learning_loop, daemon=True)
        self.learning_thread.start()
        print("ü§ñ Autonomous learning started - model will self-improve in background")
    
    def perform_global_optimization(self):
        """Perform global model optimization based on all layers' performance."""
        if not self.adaptation_enabled:
            return
        
        # Collect performance from all adaptive routers
        all_improvements = []
        for router in self.adaptive_routers:
            if router.improvement_trajectory:
                all_improvements.extend(router.improvement_trajectory)
        
        if len(all_improvements) < 3:
            return
        
        # Analyze global patterns
        recent_improvements = all_improvements[-10:]
        avg_accuracy_improvement = statistics.mean(imp.accuracy_improvement for imp in recent_improvements)
        avg_efficiency_gain = statistics.mean(imp.load_balance_improvement for imp in recent_improvements)
        
        # Update global stats
        self.autonomous_learning_stats['total_adaptations'] += 1
        if avg_accuracy_improvement > 0:
            self.autonomous_learning_stats['successful_adaptations'] += 1
        
        self.autonomous_learning_stats['performance_improvements'].append({
            'timestamp': time.time(),
            'accuracy_improvement': avg_accuracy_improvement,
            'efficiency_gain': avg_efficiency_gain
        })
        
        # Compute learning velocity
        if len(self.autonomous_learning_stats['performance_improvements']) >= 2:
            recent_improvements = self.autonomous_learning_stats['performance_improvements'][-5:]
            improvement_trend = statistics.linear_regression(
                range(len(recent_improvements)),
                [imp['accuracy_improvement'] for imp in recent_improvements]
            ).slope
            self.autonomous_learning_stats['learning_velocity'] = improvement_trend
        
        if self.improvement_logging and avg_accuracy_improvement > 0.01:
            print(f"üöÄ Global optimization: {avg_accuracy_improvement:.1%} improvement, "
                  f"learning velocity: {self.autonomous_learning_stats['learning_velocity']:.3f}")
    
    def get_improvement_summary(self) -> Dict[str, Any]:
        """Get comprehensive improvement summary."""
        total_adaptations = sum(router.adaptation_count for router in self.adaptive_routers)
        
        all_improvements = []
        for router in self.adaptive_routers:
            all_improvements.extend(router.improvement_trajectory)
        
        if not all_improvements:
            return {
                'status': 'no_adaptations_yet',
                'total_adaptations': total_adaptations
            }
        
        return {
            'status': 'actively_learning',
            'total_adaptations': total_adaptations,
            'avg_accuracy_improvement': statistics.mean(imp.accuracy_improvement for imp in all_improvements),
            'avg_efficiency_gain': statistics.mean(imp.load_balance_improvement for imp in all_improvements),
            'learning_velocity': self.autonomous_learning_stats['learning_velocity'],
            'successful_adaptation_rate': (
                self.autonomous_learning_stats['successful_adaptations'] / 
                max(self.autonomous_learning_stats['total_adaptations'], 1)
            ),
            'recent_improvements': all_improvements[-5:] if len(all_improvements) >= 5 else all_improvements
        }
    
    def stop_autonomous_learning(self):
        """Stop the autonomous learning process."""
        if self.learning_thread:
            self.stop_learning.set()
            self.learning_thread.join(timeout=5)
            print("üõë Autonomous learning stopped")


# Factory function for easy instantiation
def create_self_improving_moe(
    hidden_size: int = 768,
    num_experts: int = 8,
    num_layers: int = 6,
    enable_adaptation: bool = True
) -> SelfImprovingMoEModel:
    """Create a self-improving MoE model with autonomous learning capabilities."""
    return SelfImprovingMoEModel(
        hidden_size=hidden_size,
        num_experts=num_experts,
        num_layers=num_layers,
        adaptation_enabled=enable_adaptation,
        improvement_logging=True
    )


# Demo and testing functions
def demo_self_improving_patterns():
    """Demonstrate self-improving patterns in action."""
    print("ü§ñ Self-Improving MoE Patterns Demo")
    print("=" * 50)
    
    # Create self-improving model
    model = create_self_improving_moe(
        hidden_size=64,
        num_experts=4,
        num_layers=3,
        enable_adaptation=True
    )
    
    print("üèóÔ∏è  Created self-improving MoE model")
    print(f"   Hidden size: {model.hidden_size}")
    print(f"   Experts: {model.num_experts}")
    print(f"   Layers: {model.num_layers}")
    print(f"   Adaptation enabled: {model.adaptation_enabled}")
    print()
    
    # Simulate usage with improvement tracking
    print("üîÑ Simulating model usage with autonomous learning...")
    
    improvement_history = []
    
    for epoch in range(5):
        print(f"\nüìä Epoch {epoch + 1}/5:")
        
        # Simulate multiple forward passes
        for step in range(20):
            # Generate random input
            hidden_states = torch.randn(2, 16, model.hidden_size)
            
            # Forward pass with adaptation info
            output, adaptation_info = model(hidden_states, return_adaptation_info=True)
            
            if step % 10 == 0:
                total_adaptations = adaptation_info['total_adaptations']
                print(f"   Step {step}: {total_adaptations} total adaptations")
        
        # Get improvement summary
        summary = model.get_improvement_summary()
        improvement_history.append(summary)
        
        if summary['status'] == 'actively_learning':
            print(f"   Accuracy improvement: {summary['avg_accuracy_improvement']:.1%}")
            print(f"   Learning velocity: {summary['learning_velocity']:.3f}")
            print(f"   Success rate: {summary['successful_adaptation_rate']:.1%}")
    
    # Final performance summary
    print("\nüìà Final Self-Improvement Summary:")
    print("-" * 40)
    
    final_summary = model.get_improvement_summary()
    
    if final_summary['status'] == 'actively_learning':
        print(f"‚úÖ Model successfully adapted {final_summary['total_adaptations']} times")
        print(f"üìä Average accuracy improvement: {final_summary['avg_accuracy_improvement']:.1%}")
        print(f"‚ö° Average efficiency gain: {final_summary['avg_efficiency_gain']:.1%}")
        print(f"üöÄ Learning velocity: {final_summary['learning_velocity']:.3f}")
        print(f"üéØ Success rate: {final_summary['successful_adaptation_rate']:.1%}")
        
        print("\nüî¨ Recent Improvements:")
        for i, improvement in enumerate(final_summary['recent_improvements']):
            print(f"   {i+1}. Accuracy: {improvement.accuracy_improvement:.1%}, "
                  f"Efficiency: {improvement.load_balance_improvement:.1%}")
    else:
        print("‚è≥ Model still in initial learning phase")
    
    # Stop autonomous learning
    model.stop_autonomous_learning()
    
    # Save results
    results = {
        'model_config': {
            'hidden_size': model.hidden_size,
            'num_experts': model.num_experts,
            'num_layers': model.num_layers
        },
        'improvement_history': [
            {k: v for k, v in summary.items() if k not in ['recent_improvements']}
            for summary in improvement_history
        ],
        'final_summary': {
            k: v for k, v in final_summary.items() 
            if k not in ['recent_improvements'] and not isinstance(v, torch.Tensor)
        },
        'autonomous_learning_stats': model.autonomous_learning_stats,
        'demo_completed': True,
        'timestamp': time.time()
    }
    
    with open('self_improving_demo_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nüìÅ Results saved to self_improving_demo_results.json")
    print("\nüéâ Self-Improving Patterns Demo Complete!")
    print("    ‚úì Autonomous adaptation demonstrated")
    print("    ‚úì Performance improvement tracking")
    print("    ‚úì Background learning process")
    print("    ‚úì Pattern recognition and optimization")
    
    return results


if __name__ == "__main__":
    demo_self_improving_patterns()