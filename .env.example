# Open MoE Trainer Lab Environment Configuration

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================

# Model training settings
MODEL_NAME=custom-moe
HIDDEN_SIZE=768
NUM_EXPERTS=8
EXPERTS_PER_TOKEN=2
NUM_LAYERS=12
MAX_SEQ_LENGTH=2048

# Training hyperparameters
BATCH_SIZE=16
LEARNING_RATE=1e-4
WEIGHT_DECAY=0.01
WARMUP_STEPS=1000
NUM_EPOCHS=10
GRADIENT_ACCUMULATION_STEPS=1

# Load balancing
LOAD_BALANCE_LOSS_COEF=0.01
ROUTER_Z_LOSS_COEF=0.001

# =============================================================================
# DISTRIBUTED TRAINING
# =============================================================================

# Distributed training settings
WORLD_SIZE=1
LOCAL_RANK=0
MASTER_ADDR=localhost
MASTER_PORT=29500

# Parallelism configuration
DATA_PARALLEL_SIZE=1
EXPERT_PARALLEL_SIZE=1
MODEL_PARALLEL_SIZE=1
PIPELINE_PARALLEL_SIZE=1

# Mixed precision training
USE_AMP=true
AMP_DTYPE=float16

# =============================================================================
# DATA CONFIGURATION
# =============================================================================

# Dataset settings
DATASET_NAME=wikitext
DATASET_CONFIG=wikitext-103-raw-v1
DATA_DIR=./data
CACHE_DIR=./cache

# Preprocessing
TOKENIZER_NAME=microsoft/DialoGPT-medium
MAX_TRAIN_SAMPLES=100000
MAX_EVAL_SAMPLES=10000
PREPROCESSING_NUM_WORKERS=4

# =============================================================================
# EXPERIMENT TRACKING
# =============================================================================

# Weights & Biases
WANDB_PROJECT=open-moe-trainer-lab
WANDB_ENTITY=your-team
WANDB_API_KEY=your-wandb-api-key
WANDB_RUN_NAME=
WANDB_TAGS=moe,training
WANDB_NOTES=

# TensorBoard
TENSORBOARD_LOG_DIR=./logs/tensorboard

# MLflow (optional)
MLFLOW_TRACKING_URI=
MLFLOW_EXPERIMENT_NAME=moe-experiments

# =============================================================================
# MODEL CHECKPOINTING
# =============================================================================

# Checkpoint settings
CHECKPOINT_DIR=./checkpoints
SAVE_STRATEGY=steps
SAVE_STEPS=1000
SAVE_TOTAL_LIMIT=3
LOAD_BEST_MODEL_AT_END=true

# Model output
OUTPUT_DIR=./outputs
OVERWRITE_OUTPUT_DIR=true

# =============================================================================
# INFERENCE CONFIGURATION
# =============================================================================

# Inference optimization
USE_EXPERT_CACHE=true
EXPERT_CACHE_SIZE_GB=8
CACHE_POLICY=lru
PRELOAD_TOP_K_EXPERTS=4

# Quantization
USE_QUANTIZATION=false
QUANTIZATION_METHOD=gptq
QUANTIZATION_BITS=4
QUANTIZE_ROUTER=false

# Compilation
USE_TORCH_COMPILE=false
COMPILE_BACKEND=inductor

# =============================================================================
# MONITORING & OBSERVABILITY
# =============================================================================

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE=./logs/training.log

# Metrics collection
COLLECT_ROUTING_METRICS=true
ROUTING_METRICS_INTERVAL=100
EXPERT_UTILIZATION_TRACKING=true

# Performance monitoring
PROFILE_MEMORY=false
PROFILE_COMPUTE=false
MONITOR_GPU_UTILIZATION=true

# Health checks
HEALTH_CHECK_INTERVAL=60
ENABLE_WATCHDOG=true

# =============================================================================
# DEVELOPMENT & DEBUGGING
# =============================================================================

# Debug settings
DEBUG=false
TORCH_DISTRIBUTED_DEBUG=DETAIL
CUDA_LAUNCH_BLOCKING=0
TORCH_AUTOGRAD_PROFILER=false

# Testing
RUN_SLOW_TESTS=false
TEST_TIMEOUT=300

# Visualization
ENABLE_DASHBOARD=true
DASHBOARD_PORT=8080
DASHBOARD_HOST=0.0.0.0

# =============================================================================
# CLOUD & DEPLOYMENT
# =============================================================================

# Cloud storage
CLOUD_PROVIDER=aws
AWS_REGION=us-west-2
AWS_S3_BUCKET=your-moe-bucket
AZURE_STORAGE_ACCOUNT=
GCS_BUCKET=

# Container settings
DOCKER_IMAGE_TAG=latest
DOCKER_REGISTRY=your-registry

# Kubernetes (optional)
K8S_NAMESPACE=moe-training
K8S_SERVICE_ACCOUNT=moe-trainer

# =============================================================================
# SECURITY
# =============================================================================

# API keys and secrets
HF_TOKEN=your-huggingface-token
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# Encryption
ENCRYPT_CHECKPOINTS=false
ENCRYPTION_KEY=

# Access control
ALLOWED_USERS=
API_RATE_LIMIT=100

# =============================================================================
# HARDWARE OPTIMIZATION
# =============================================================================

# GPU settings
CUDA_VISIBLE_DEVICES=0,1,2,3
GPU_MEMORY_FRACTION=0.9
ENABLE_MEMORY_POOL=true

# CPU settings
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8

# Memory management
MAX_MEMORY_GB=32
SWAP_EXPERTS_TO_CPU=false

# =============================================================================
# BENCHMARKING
# =============================================================================

# Benchmark settings
BENCHMARK_MODE=false
BENCHMARK_ITERATIONS=100
BENCHMARK_WARMUP_ITERATIONS=10

# Performance targets
TARGET_TOKENS_PER_SECOND=1000
TARGET_INFERENCE_LATENCY_MS=100
TARGET_MEMORY_EFFICIENCY=0.8