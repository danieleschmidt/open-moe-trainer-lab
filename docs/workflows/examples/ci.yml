name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Code Quality Checks
  quality:
    name: "Code Quality"
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Code Formatting Check
        run: |
          black --check --diff moe_lab/ tests/ examples/
          isort --check-only --diff moe_lab/ tests/ examples/

      - name: Linting
        run: |
          pylint moe_lab/ tests/
          ruff check moe_lab/ tests/ examples/

      - name: Type Checking
        run: mypy moe_lab/

      - name: Security Linting
        run: bandit -r moe_lab/ -f json -o bandit-report.json || true

      - name: Upload Bandit Report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: bandit-report
          path: bandit-report.json

  # Unit Tests
  test-unit:
    name: "Unit Tests"
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]
        
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,gpu]

      - name: Run Unit Tests
        run: |
          pytest tests/unit/ \
            -v \
            --cov=moe_lab \
            --cov-report=xml \
            --cov-report=term-missing \
            --junitxml=junit-${{ matrix.python-version }}.xml

      - name: Upload Coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.python-version == '3.11'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-unit

      - name: Upload Test Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: junit-${{ matrix.python-version }}.xml

  # Integration Tests
  test-integration:
    name: "Integration Tests"
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: moelab_test
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test123
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,gpu]

      - name: Wait for Services
        run: |
          timeout 60 bash -c 'until redis-cli -h localhost ping; do sleep 1; done'
          timeout 60 bash -c 'until pg_isready -h localhost -U test; do sleep 1; done'

      - name: Run Integration Tests
        env:
          REDIS_URL: redis://localhost:6379
          DATABASE_URL: postgresql://test:test123@localhost:5432/moelab_test
        run: |
          pytest tests/integration/ \
            -v \
            --cov=moe_lab \
            --cov-append \
            --cov-report=xml \
            --junitxml=junit-integration.xml

      - name: Upload Coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: integration
          name: codecov-integration

  # Build and Package Tests
  build:
    name: "Build & Package"
    runs-on: ubuntu-latest
    needs: [quality, test-unit]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for proper versioning

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Build Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine

      - name: Build Package
        run: python -m build

      - name: Check Package
        run: twine check dist/*

      - name: Test Package Installation
        run: |
          pip install dist/*.whl
          python -c "import moe_lab; print(moe_lab.__version__)"

      - name: Upload Package Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: python-package
          path: dist/

  # Docker Build Test
  docker:
    name: "Docker Build"
    runs-on: ubuntu-latest
    needs: [quality, test-unit]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Development Image
        uses: docker/build-push-action@v5
        with:
          context: .
          target: development
          cache-from: type=gha
          cache-to: type=gha,mode=max
          load: true
          tags: open-moe-trainer:dev-test

      - name: Test Development Image
        run: |
          docker run --rm open-moe-trainer:dev-test python -c "import moe_lab; print('Import successful')"

      - name: Build Production Image
        uses: docker/build-push-action@v5
        with:
          context: .
          target: production
          cache-from: type=gha
          cache-to: type=gha,mode=max
          load: true
          tags: open-moe-trainer:prod-test

      - name: Test Production Image
        run: |
          docker run --rm open-moe-trainer:prod-test python -c "import moe_lab; print('Production build successful')"

      - name: Run Container Security Scan
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'open-moe-trainer:prod-test'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # End-to-End Tests (only on main branch)
  test-e2e:
    name: "E2E Tests"
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: [test-integration, build, docker]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Start Services
        run: |
          docker-compose up -d redis postgres
          sleep 10

      - name: Run E2E Tests
        env:
          REDIS_URL: redis://localhost:6379
          DATABASE_URL: postgresql://moelab:moelab123@localhost:5432/moelab
        run: |
          pytest tests/e2e/ \
            -v \
            --junitxml=junit-e2e.xml \
            -m "not slow"

      - name: Upload E2E Test Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-test-results
          path: junit-e2e.xml

      - name: Cleanup
        if: always()
        run: docker-compose down -v

  # Documentation Build
  docs:
    name: "Documentation"
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' && contains(github.event.pull_request.changed_files.*.filename, 'docs/')
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]

      - name: Build Documentation
        run: |
          cd docs
          sphinx-build -b html source build -W

      - name: Upload Documentation
        uses: actions/upload-artifact@v3
        with:
          name: documentation
          path: docs/build/

  # Performance Tests (only on main, nightly)
  performance:
    name: "Performance Tests"
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event_name == 'schedule'
    needs: [test-integration, build]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev,benchmarking]

      - name: Run Performance Tests
        run: |
          pytest tests/benchmarks/ \
            -v \
            --benchmark-only \
            --benchmark-json=benchmark-results.json

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark-results.json

      - name: Performance Regression Check
        run: |
          python scripts/check_performance_regression.py \
            --current benchmark-results.json \
            --baseline performance-baseline.json \
            --threshold 0.1

  # Status Check Summary
  ci-success:
    name: "CI Success"
    runs-on: ubuntu-latest
    needs: [quality, test-unit, test-integration, build, docker]
    if: always()
    
    steps:
      - name: Check Status
        run: |
          if [[ "${{ needs.quality.result }}" == "success" && \
                "${{ needs.test-unit.result }}" == "success" && \
                "${{ needs.test-integration.result }}" == "success" && \
                "${{ needs.build.result }}" == "success" && \
                "${{ needs.docker.result }}" == "success" ]]; then
            echo "All required checks passed!"
            exit 0
          else
            echo "Some required checks failed!"
            exit 1
          fi

      - name: Report Status
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [[ -n "$SLACK_WEBHOOK_URL" ]]; then
            STATUS="${{ job.status }}"
            COLOR="good"
            [[ "$STATUS" != "success" ]] && COLOR="danger"
            
            curl -X POST "$SLACK_WEBHOOK_URL" \
              -H 'Content-type: application/json' \
              --data "{
                \"attachments\": [{
                  \"color\": \"$COLOR\",
                  \"title\": \"CI Pipeline - ${{ github.repository }}\",
                  \"text\": \"Branch: ${{ github.ref_name }}\nCommit: ${{ github.sha }}\nStatus: $STATUS\",
                  \"footer\": \"GitHub Actions\",
                  \"ts\": $(date +%s)
                }]
              }"
          fi